{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2s2e/AppleProject/blob/main/Steven_Copy_of_ngram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deMZjlbXD7l-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d182025-46a3-4d55-defa-e2f25c4fd304"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cuqRY9dD7mA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Add any `import`s here.\n",
        "# You may import anything from the Python Standard Library (auto-grader uses python 3.10)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auYGpEzwD7mB"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "class NGramModel:\n",
        "    # Special tokens\n",
        "    START_TOKEN = \"<start>\"\n",
        "    STOP_TOKEN = \"<stop>\"\n",
        "    UNK_TOKEN = \"<unk>\"\n",
        "\n",
        "    def __init__(self, n: int, laplace_smoothing: bool = False) -> None:\n",
        "        \"\"\"Initialize the N-gram model.\n",
        "\n",
        "        Args:\n",
        "            n (int): The 'n' in the N-gram model. E.g., n=3 constructs a trigram model. We will only test `n in (1, 2, 3)`.\n",
        "            laplace_smoothing (bool, optional): Wether to use Laplace smoothing. Defaults to False.\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.laplace_smoothing = laplace_smoothing\n",
        "\n",
        "\n",
        "\n",
        "        # Initialize variables here as you see fit.\n",
        "        self.documents = []\n",
        "        self.vocab = defaultdict(int)\n",
        "        self.ngrams = defaultdict(int)\n",
        "        self.ngram_counts = defaultdict(int)\n",
        "        self.unknown_words = set()\n",
        "        self.total_tokens = 0;\n",
        "\n",
        "    def fit(self, dataset: list[list[str]]) -> None:\n",
        "        \"\"\"Build the N-gram model from a dataset.\n",
        "\n",
        "        The dataset consists of a list of documents, where\n",
        "        each document is made up of a list of words followed by a STOP_TOKEN.\n",
        "\n",
        "        Args:\n",
        "            dataset (list[list[str]]): The training set.\n",
        "        \"\"\"\n",
        "\n",
        "        #build the vocab, and add our start tokens\n",
        "        for i in range(len(dataset)):\n",
        "            self.documents.append(dataset[i])\n",
        "            for j in range(len(dataset[i])):\n",
        "                self.vocab[dataset[i][j]] += 1\n",
        "            #self.documents[i].insert(0, NGramModel.START_TOKEN)\n",
        "\n",
        "        #find unknown words\n",
        "        self.vocab[NGramModel.UNK_TOKEN] = 0\n",
        "        for word in self.vocab:\n",
        "            if self.vocab[word] < 3:\n",
        "                self.unknown_words.add(word)\n",
        "                #replaceing the # of occurences of unknown word with unk\n",
        "                self.vocab[NGramModel.UNK_TOKEN] += self.vocab[word]\n",
        "\n",
        "        for unk in self.unknown_words:\n",
        "            del self.vocab[unk]\n",
        "\n",
        "        #convert tokens to unks\n",
        "        for i in range(len(self.documents)):\n",
        "            for j in range(len(self.documents[i])):\n",
        "                if self.documents[i][j] in self.unknown_words:\n",
        "                    self.documents[i][j] = NGramModel.UNK_TOKEN\n",
        "\n",
        "        #build n-grams\n",
        "        for i in range(len(self.documents)):\n",
        "            doc_copy = self.documents[i].copy()\n",
        "            #add starts\n",
        "            for j in range(self.n - 1):\n",
        "                doc_copy.insert(0, NGramModel.START_TOKEN)\n",
        "\n",
        "            #build ngrams\n",
        "            for k in range(self.n - 1, len(doc_copy)):\n",
        "                ngram = tuple(doc_copy[k - self.n + 1:k + 1])\n",
        "                self.ngrams[ngram] += 1\n",
        "\n",
        "\n",
        "        self.total_tokens = sum(self.vocab.values())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        print(len(self.vocab))\n",
        "\n",
        "\n",
        "    def get_probability(self, ngram: list[str]) -> float:\n",
        "        \"\"\"Calculate the probability of an N-gram.\n",
        "\n",
        "        For example, for `n=3` (trigram model),\n",
        "        `get_probability(['Hello', 'world', STOP_TOKEN])`\n",
        "        should return p(x_3=STOP_TOKEN|x_1='Hello', x_2='world').\n",
        "\n",
        "        You may assume the N-gram only contains words from the vocabulary plus special tokens.\n",
        "\n",
        "        Args:\n",
        "            ngram (list[str]): The N-gram as a list of words. Must be `n` long.\n",
        "\n",
        "        Returns:\n",
        "            float: The probability of the N-gram.\n",
        "        \"\"\"\n",
        "        if self.n == 1:\n",
        "          return self.vocab[ngram[0]] / self.total_tokens\n",
        "\n",
        "        numerator = 0\n",
        "        denomerator = 0\n",
        "        ngrams_list = list(self.ngrams.keys())\n",
        "        ngram = tuple(ngram)\n",
        "\n",
        "\n",
        "        for i in range(len(self.ngrams)):\n",
        "          target_ngram = ngrams_list[i]\n",
        "\n",
        "          #exact match of the ngram\n",
        "          if target_ngram == ngram:\n",
        "            numerator = self.ngrams[target_ngram]\n",
        "          #given is true\n",
        "          if ngram[0:self.n - 1] == target_ngram[0:self.n - 1]:\n",
        "            denomerator += self.ngrams[target_ngram]\n",
        "\n",
        "        if self.laplace_smoothing:\n",
        "          numerator += 1\n",
        "          denomerator += len(self.vocab)\n",
        "\n",
        "        return numerator / denomerator\n",
        "\n",
        "\n",
        "       # raise NotImplementedError('To be implemented in 1.1.1')\n",
        "\n",
        "    def get_perplexity(self, doc: list[str]) -> float:\n",
        "        \"\"\"Calculates the perplexity of a sequence.\n",
        "\n",
        "        Args:\n",
        "            doc (list[str]): A sequence of words.\n",
        "\n",
        "        Returns:\n",
        "            float: The perplexity of the sequence.\n",
        "        \"\"\"\n",
        "        doc_ngrams = []\n",
        "\n",
        "        #pad with start\n",
        "        doc_copy = doc.copy()\n",
        "        for j in range(self.n - 1):\n",
        "            doc_copy.insert(0, NGramModel.START_TOKEN)\n",
        "\n",
        "        #convert the doc into a list of ngrams\n",
        "        for i in range(len(doc)):\n",
        "          ngram = tuple(doc_copy[i:i + self.n])\n",
        "          doc_ngrams.append(ngram)\n",
        "          print(ngram)\n",
        "\n",
        "        #add together\n",
        "        log_p_s = 0\n",
        "        for ngram in doc_ngrams:\n",
        "          prob = self.get_probability(ngram)\n",
        "          log_p_s += np.log(prob)\n",
        "\n",
        "        return np.exp(-log_p_s / (len(doc)))\n",
        "\n",
        "\n",
        "        #raise NotImplementedError('To be implemented in 1.1.2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2hOzFARD7mB"
      },
      "outputs": [],
      "source": [
        "# Read the training set, split the documents into lists of words, and adding a STOP_TOKEN to the end of each document.\n",
        "\n",
        "with open('1b_benchmark.train.tokens', 'r', encoding='utf-8') as file:\n",
        "    dataset = [line.split() + [NGramModel.STOP_TOKEN] for line in file]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6S37GavD7mB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb941350-dd05-4e9b-e4e0-1403d30e5813"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Having',\n",
              " 'a',\n",
              " 'little',\n",
              " 'flexibility',\n",
              " 'on',\n",
              " 'that',\n",
              " 'issue',\n",
              " 'would',\n",
              " 'go',\n",
              " 'a',\n",
              " 'long',\n",
              " 'way',\n",
              " 'to',\n",
              " 'putting',\n",
              " 'together',\n",
              " 'a',\n",
              " 'final',\n",
              " 'package',\n",
              " '.',\n",
              " '<stop>']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "# This is what the first document looks like.\n",
        "\n",
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUUWg5QBD7mC"
      },
      "source": [
        "### 1.1.1 Implement `fit` and `get_probability`\n",
        "\n",
        "Implement the methods marked with \"To be implemented in 1.1.1.\"\n",
        "For now you can ignore `laplace_smoothing` (you will implement that in 1.1.3).\n",
        "\n",
        "Once you are done, the below should work.\n",
        "This is roughly how we will test your implementation, though we will use a different training set and a few more test cases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYwD9sP9D7mC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "326483f0-36cf-4538-9e4b-98a8d7e5bebe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26602\n"
          ]
        }
      ],
      "source": [
        "unigram = NGramModel(n=1)\n",
        "unigram.fit(dataset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "p = unigram.get_probability([\"a\"])\n",
        "print(p)\n",
        "assert np.allclose(p, 0.019381910832735126)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWiKOvTxXuyh",
        "outputId": "3cf92586-4ec9-4098-f488-ccbffdcc1c0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.019381910832735126\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zmAS4WHaD7mD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8451350e-ff53-4d9e-f032-6658d67866d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26601\n"
          ]
        }
      ],
      "source": [
        "trigram = NGramModel(n=3)\n",
        "trigram.fit(dataset)\n",
        "\n",
        "p = trigram.get_probability([\"a\", \"little\", \"flexibility\"])\n",
        "assert np.allclose(p, 0.006060606060606061)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPRBspvbD7mD"
      },
      "source": [
        "### 1.1.2 Implement `get_perplexity`\n",
        "\n",
        "Implement the methods marked with \"To be implemented in 1.1.2.\"\n",
        "\n",
        "Once you are done, the below should work.\n",
        "At this point, your implementation likely can't handle sequences with unseen N-grams. You will fix that in the next part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0j1T8KvJD7mD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "outputId": "07cb8bd9-fcae-4241-f875-3914aa632ff2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26601\n",
            "('Having',)\n",
            "('a',)\n",
            "('little',)\n",
            "('flexibility',)\n",
            "('on',)\n",
            "('that',)\n",
            "('issue',)\n",
            "('would',)\n",
            "('go',)\n",
            "('a',)\n",
            "('long',)\n",
            "('way',)\n",
            "('to',)\n",
            "('putting',)\n",
            "('together',)\n",
            "('a',)\n",
            "('final',)\n",
            "('package',)\n",
            "('.',)\n",
            "('<stop>',)\n",
            "887.5482553542884\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-1bb07beaeb16>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mperplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munigram\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m924.3003136958376\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "unigram = NGramModel(n=1)\n",
        "unigram.fit(dataset)\n",
        "\n",
        "perplexity = unigram.get_perplexity(dataset[0])\n",
        "print(perplexity)\n",
        "assert np.allclose(perplexity, 924.3003136958376)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NivnY6_D7mD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "464b1952-336d-442f-f656-79bb9a67c3aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26601\n",
            "('<start>', '<start>', 'Having')\n",
            "('<start>', 'Having', 'a')\n",
            "('Having', 'a', 'little')\n",
            "('a', 'little', 'flexibility')\n",
            "('little', 'flexibility', 'on')\n",
            "('flexibility', 'on', 'that')\n",
            "('on', 'that', 'issue')\n",
            "('that', 'issue', 'would')\n",
            "('issue', 'would', 'go')\n",
            "('would', 'go', 'a')\n",
            "('go', 'a', 'long')\n",
            "('a', 'long', 'way')\n",
            "('long', 'way', 'to')\n",
            "('way', 'to', 'putting')\n",
            "('to', 'putting', 'together')\n",
            "('putting', 'together', 'a')\n",
            "('together', 'a', 'final')\n",
            "('a', 'final', 'package')\n",
            "('final', 'package', '.')\n",
            "('package', '.', '<stop>')\n"
          ]
        }
      ],
      "source": [
        "trigram = NGramModel(n=3)\n",
        "trigram.fit(dataset)\n",
        "\n",
        "perplexity = trigram.get_perplexity(dataset[0])\n",
        "assert np.allclose(perplexity, 8.42398352642198)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBwjuROzD7mE"
      },
      "source": [
        "### 1.1.3 Implement Laplace Smoothing\n",
        "\n",
        "Modify you implementation such that when `laplace_smoothing=True`,\n",
        "`get_probability` and `get_perplexity` return probability and perplexity with Laplace smoothing.\n",
        "\n",
        "After you are done, the below should work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pel5TEeD7mE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9440dc8d-09d6-4df1-d5b1-984a9df16bd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26601\n"
          ]
        }
      ],
      "source": [
        "trigram = NGramModel(n=3, laplace_smoothing=True)\n",
        "trigram.fit(dataset)\n",
        "\n",
        "# `[\"hello\", \"world\", \"!\"]` does not exist in the training set,\n",
        "# but with Laplace smoothing this should now have a small probability.\n",
        "p = trigram.get_probability([\"hello\", \"world\", \"!\"])\n",
        "assert np.allclose(p, 3.7591158559506806e-05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gmp85enhD7mE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "51d1b724-712e-4e1f-f991-0849edc2b5c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26601\n",
            "('<start>', '<start>', 'This')\n",
            "('<start>', 'This', 'sequence')\n",
            "('This', 'sequence', 'contains')\n",
            "('sequence', 'contains', 'unseen')\n",
            "('contains', 'unseen', 'trigrams')\n",
            "('unseen', 'trigrams', '!')\n",
            "('trigrams', '!', '<stop>')\n",
            "3836.067510421336\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-f8e52d4ce2dd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7711.325324297971\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trigram = NGramModel(n=3, laplace_smoothing=True)\n",
        "trigram.fit(dataset)\n",
        "\n",
        "# Without Laplace smoothing you can't compute perplexity for this sequence because of the unseen N-grams.\n",
        "perplexity = trigram.get_perplexity(\n",
        "    [\"This\", \"sequence\", \"contains\", \"unseen\", \"trigrams\", \"!\", NGramModel.STOP_TOKEN]\n",
        ")\n",
        "print(perplexity)\n",
        "assert np.allclose(perplexity, 7711.325324297971)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKu_qjJuD7mE"
      },
      "source": [
        "### Write-up\n",
        "\n",
        "Add any write-up question related code below."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "cse256-hw1",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}